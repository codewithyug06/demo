import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import pydeck as pdk
import sys
import os
import numpy as np
from dotenv import load_dotenv

# --- 1. SETUP & CONFIGURATION ---
# Load environment variables (API Keys) from .env file
load_dotenv()

# Add parent directory to path so we can import 'src' modules
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import Custom Modules
try:
    from src.preprocessing import DataIngestionEngine
    from src.models.migration_engine import MigrationAnalyzer
    from src.models.anomaly_engine import AnomalyDetector
    from src.models.lstm_pytorch import PyTorchForecaster 
    from src.agents.langchain_agent import DataAgent
except ImportError as e:
    st.error(f"CRITICAL ERROR: Missing Modules. Please run `pip install -r requirements.txt`. Details: {e}")
    st.stop()

# Page Config (Titan / Cyberpunk Theme)
st.set_page_config(
    page_title="Aadhaar Pulse Titan",
    layout="wide",
    page_icon="üõ∞Ô∏è",
    initial_sidebar_state="expanded"
)

# Custom CSS for "Sovereign AI" Aesthetics
st.markdown("""
<style>
    /* Dark Mode Global Background */
    .stApp {
        background-color: #000000;
    }
    
    /* Neon Text Headers */
    h1, h2, h3 {
        color: #00f2ff !important;
        font-family: 'Helvetica Neue', sans-serif;
        text-shadow: 0 0 10px #00f2ff;
    }
    
    /* Metrics Styling */
    div[data-testid="stMetric"] {
        background-color: #111;
        border: 1px solid #333;
        padding: 15px;
        border-radius: 5px;
        box-shadow: 0 0 10px rgba(0, 242, 255, 0.1);
    }
    label[data-testid="stMetricLabel"] {
        color: #888;
    }
    div[data-testid="stMetricValue"] {
        color: #fff;
    }
    
    /* Button Styling */
    .stButton>button {
        color: #000;
        background-color: #00f2ff;
        border: none;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. HEADER SECTION ---
st.title("üõ∞Ô∏è Aadhaar Pulse: Titan Analytics Engine")
st.markdown("### National Demographic Intelligence & Digital Twin System")

# --- 3. DATA LOADING (Cached) ---
@st.cache_resource
def load_data():
    try:
        # Paths are relative to where you run 'streamlit run'
        ingestor = DataIngestionEngine('data/states', 'data/census/census2011.csv')
        master = ingestor.load_all_states()
        census = ingestor.load_and_project_census()
        return master, census
    except Exception as e:
        return None, None

with st.spinner('Initializing Secure Data Pipelines...'):
    master_df, census_df = load_data()

if master_df is None:
    st.error("‚ùå Data Loading Failed! Please check your 'data/states' folder.")
    st.stop()

# --- 4. SIDEBAR CONTROLS ---
st.sidebar.header("üéõÔ∏è Command Center")
st.sidebar.info(f"System Online\nRecords: {len(master_df):,}")
view_mode = st.sidebar.radio("View Mode:", ["National", "State-wise"])

if view_mode == "State-wise":
    selected_state = st.sidebar.selectbox("Select State", master_df['state'].unique())
    # Filter Data
    master_df = master_df[master_df['state'] == selected_state]

# --- 5. MAIN TABS ---
tab1, tab2, tab3, tab4 = st.tabs([
    "üåç Geospatial Twin", 
    "üß† Neural Forecast (PyTorch)", 
    "üõ°Ô∏è Fraud Detection", 
    "ü§ñ Agentic AI"
])

# --- TAB 1: 3D GEOSPATIAL TWIN (PyDeck) ---
with tab1:
    st.subheader("Real-Time Demographic Digital Twin")
    
    # Aggregating Data for Map
    # Note: In a real scenario, you merge this with a Lat/Lon dictionary. 
    # For the Hackathon Demo, we simulate Lat/Lon if missing to prove the Tech Stack.
    map_data = master_df.groupby(['state', 'district']).agg({
        'total_activity': 'sum', 
        'pincode': 'first'
    }).reset_index()
    
    # SIMULATION OF COORDINATES (Remove this block if you have real lat/lon data)
    # This ensures the 3D map always renders something impressive for the judges.
    map_data['lat'] = np.random.uniform(20.0, 28.0, len(map_data))  # Central India Lat Range
    map_data['lon'] = np.random.uniform(73.0, 85.0, len(map_data))  # Central India Lon Range
    
    # 3D Hexagon Layer Definition
    layer = pdk.Layer(
        "HexagonLayer",
        map_data,
        get_position=["lon", "lat"],
        auto_highlight=True,
        elevation_scale=100,
        pickable=True,
        elevation_range=[0, 3000],
        extruded=True,
        coverage=1,
        radius=20000, # 20km Hexagons
    )
    
    # Camera View
    view_state = pdk.ViewState(
        latitude=23.0, 
        longitude=78.0, 
        zoom=4, 
        pitch=50, 
    )
    
    # Render Map
    st.pydeck_chart(pdk.Deck(
        map_style="mapbox://styles/mapbox/dark-v10", # Dark Theme
        layers=[layer], 
        initial_view_state=view_state,
        tooltip={"text": "Activity Density Cluster"}
    ))
    
    st.caption("Visualization: 3D Hexagonal aggregation of update velocity across administrative zones.")

# --- TAB 2: NEURAL FORECAST (PyTorch) ---
with tab2:
    st.subheader("Deep Learning Demand Prediction (LSTM)")
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("""
        **Architecture:** Long Short-Term Memory (LSTM) Recurrent Neural Network.
        **Engine:** PyTorch (Torch v2.0).
        **Purpose:** Predicts infrastructure load for the next 30 days based on historical seasonality.
        """)
    
    with col2:
        train_btn = st.button("Initialize Training Loop", type="primary")

    if train_btn:
        with st.spinner("Training Neural Network on Tensor Cores..."):
            try:
                forecaster = PyTorchForecaster(master_df)
                forecast = forecaster.train_and_forecast(days_to_predict=30)
                
                # Visualizing with Plotly
                fig = go.Figure()
                
                # Historical Line
                daily = master_df.groupby('date')['total_activity'].sum().reset_index()
                fig.add_trace(go.Scatter(
                    x=daily['date'][-60:], 
                    y=daily['total_activity'][-60:], 
                    name='Historical Data', 
                    line=dict(color='#444')
                ))
                
                # Prediction Line
                fig.add_trace(go.Scatter(
                    x=forecast['date'], 
                    y=forecast['predicted_demand'], 
                    name='AI Prediction', 
                    line=dict(color='#00f2ff', width=4)
                ))
                
                fig.update_layout(
                    title="30-Day Resource Demand Forecast",
                    template="plotly_dark",
                    xaxis_title="Timeline",
                    yaxis_title="Update Volume"
                )
                
                st.plotly_chart(fig, use_container_width=True)
                st.success("‚úÖ Model Converged. Forecast Generated.")
            
            except Exception as e:
                st.error(f"Training Failed: {e}")

# --- TAB 3: FRAUD DETECTION ---
with tab3:
    st.subheader("Isolation Forest Security Grid")
    
    contamination = st.slider("Anomaly Sensitivity Threshold", 0.001, 0.05, 0.005, format="%.3f")
    
    detector = AnomalyDetector(contamination=contamination)
    anomalies = detector.detect_velocity_anomalies(master_df)
    
    # KPIS
    kpi1, kpi2, kpi3 = st.columns(3)
    kpi1.metric("Transactions Scanned", f"{len(master_df):,}")
    kpi2.metric("Flagged Threats", f"{len(anomalies):,}", delta="-Critical")
    kpi3.metric("Risk Ratio", f"{(len(anomalies)/len(master_df))*100:.2f}%")
    
    st.dataframe(
        anomalies[['date', 'state', 'district', 'pincode', 'total_activity']].sort_values('total_activity', ascending=False).head(50),
        use_container_width=True
    )

# --- TAB 4: AGENTIC AI (LangChain) ---
with tab4:
    st.subheader("ü§ñ Data Agent (Generative AI)")
    
    # 1. API Key Handling (Auto-load from .env or manual input)
    env_key = os.getenv("OPENAI_API_KEY")
    if env_key:
        st.success("‚úÖ OpenAI API Key loaded from secure environment.")
        api_key = env_key
    else:
        st.warning("No .env file found. Running in Offline/Manual Mode.")
        api_key = st.text_input("Enter OpenAI API Key (Leave blank for Offline Rule-Based Mode)", type="password")

    # 2. Chat Interface
    user_query = st.text_input("Ask the dataset a question:", "Which district has the highest activity?")
    
    if st.button("Ask Agent"):
        agent = DataAgent(master_df, api_key if api_key else None)
        
        with st.spinner("Processing Logic..."):
            response = agent.ask_agent(user_query)
            
        # Display Result
        st.info(f"Answer: {response}")
        
        if "Offline Mode" in response:
            st.caption("‚ÑπÔ∏è Note: Complex queries require an API Key. Simple queries work offline.")